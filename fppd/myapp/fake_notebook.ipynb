{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"twitter_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Dictionary with city, country, and timezone data\n",
    "city_info = {\n",
    "    \"New York\": {\"country\": \"United States\", \"timezone\": \"GMT-5\"},\n",
    "    \"Los Angeles\": {\"country\": \"United States\", \"timezone\": \"GMT-8\"},\n",
    "    \"London\": {\"country\": \"United Kingdom\", \"timezone\": \"GMT+0\"},\n",
    "    \"Tokyo\": {\"country\": \"Japan\", \"timezone\": \"GMT+9\"},\n",
    "    \"Beijing\": {\"country\": \"China\", \"timezone\": \"GMT+8\"},\n",
    "    \"Moscow\": {\"country\": \"Russia\", \"timezone\": \"GMT+3\"},\n",
    "    \"Sydney\": {\"country\": \"Australia\", \"timezone\": \"GMT+11\"},\n",
    "    \"Paris\": {\"country\": \"France\", \"timezone\": \"GMT+1\"},\n",
    "    \"Berlin\": {\"country\": \"Germany\", \"timezone\": \"GMT+1\"},\n",
    "    \"Rio de Janeiro\": {\"country\": \"Brazil\", \"timezone\": \"GMT-3\"},\n",
    "    \"Cape Town\": {\"country\": \"South Africa\", \"timezone\": \"GMT+2\"},\n",
    "    \"Mumbai\": {\"country\": \"India\", \"timezone\": \"GMT+5:30\"},\n",
    "    \"Dubai\": {\"country\": \"United Arab Emirates\", \"timezone\": \"GMT+4\"},\n",
    "    \"Singapore\": {\"country\": \"Singapore\", \"timezone\": \"GMT+8\"},\n",
    "    \"Seoul\": {\"country\": \"South Korea\", \"timezone\": \"GMT+9\"},\n",
    "    \"Mexico City\": {\"country\": \"Mexico\", \"timezone\": \"GMT-6\"},\n",
    "    \"Buenos Aires\": {\"country\": \"Argentina\", \"timezone\": \"GMT-3\"},\n",
    "    \"Rome\": {\"country\": \"Italy\", \"timezone\": \"GMT+1\"},\n",
    "    \"Istanbul\": {\"country\": \"Turkey\", \"timezone\": \"GMT+3\"},\n",
    "    \"Jakarta\": {\"country\": \"Indonesia\", \"timezone\": \"GMT+7\"},\n",
    "    \"Bangkok\": {\"country\": \"Thailand\", \"timezone\": \"GMT+7\"},\n",
    "    \"Kuala Lumpur\": {\"country\": \"Malaysia\", \"timezone\": \"GMT+8\"},\n",
    "    \"Cairo\": {\"country\": \"Egypt\", \"timezone\": \"GMT+2\"},\n",
    "    \"Hong Kong\": {\"country\": \"China\", \"timezone\": \"GMT+8\"},\n",
    "    \"SÃ£o Paulo\": {\"country\": \"Brazil\", \"timezone\": \"GMT-3\"},\n",
    "    \"Lagos\": {\"country\": \"Nigeria\", \"timezone\": \"GMT+1\"},\n",
    "    \"Barcelona\": {\"country\": \"Spain\", \"timezone\": \"GMT+1\"},\n",
    "    \"Zurich\": {\"country\": \"Switzerland\", \"timezone\": \"GMT+1\"},\n",
    "    \"Stockholm\": {\"country\": \"Sweden\", \"timezone\": \"GMT+1\"},\n",
    "    \"Oslo\": {\"country\": \"Norway\", \"timezone\": \"GMT+1\"},\n",
    "    \"Copenhagen\": {\"country\": \"Denmark\", \"timezone\": \"GMT+1\"},\n",
    "    \"Brussels\": {\"country\": \"Belgium\", \"timezone\": \"GMT+1\"},\n",
    "    \"Vienna\": {\"country\": \"Austria\", \"timezone\": \"GMT+1\"},\n",
    "    \"Amsterdam\": {\"country\": \"Netherlands\", \"timezone\": \"GMT+1\"},\n",
    "    \"Warsaw\": {\"country\": \"Poland\", \"timezone\": \"GMT+1\"},\n",
    "    \"Prague\": {\"country\": \"Czech Republic\", \"timezone\": \"GMT+1\"},\n",
    "    \"Budapest\": {\"country\": \"Hungary\", \"timezone\": \"GMT+1\"},\n",
    "    \"Dublin\": {\"country\": \"Ireland\", \"timezone\": \"GMT+0\"},\n",
    "    \"Edinburgh\": {\"country\": \"United Kingdom\", \"timezone\": \"GMT+0\"},\n",
    "}\n",
    "\n",
    "# Create a list of all valid city-country-timezone combinations\n",
    "cities = list(city_info.keys())\n",
    "\n",
    "# Load your existing CSV file (replace with your actual file name)\n",
    "df = pd.read_csv(\"dataset_file.csv\")\n",
    "\n",
    "# Generate random city assignments\n",
    "random_cities = random.choices(cities, k=len(df))\n",
    "\n",
    "# Create new columns using the random city assignments\n",
    "df[\"location_city\"] = random_cities\n",
    "df[\"location_country\"] = df[\"location_city\"].apply(lambda city: city_info[city][\"country\"])\n",
    "df[\"timezone\"] = df[\"location_city\"].apply(lambda city: city_info[city][\"timezone\"])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"updated_city_file.csv\", index=False)\n",
    "\n",
    "print(\"All rows have been successfully updated with random city, country, and timezone data.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open('twitter_dataset.csv','rb') as file:\n",
    "    result=chardet.detect(file.read())\n",
    "    print(f\"{result['encoding']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier:0.5008478802992519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix\n",
    "\n",
    "#defining features and target\n",
    "features=[ 'verified', 'friends_count', 'followers_count', 'gender:confidence']\n",
    "target='status'\n",
    "\n",
    "x=data[features]\n",
    "y=data[target]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.5,random_state=42,stratify=y)\n",
    "\n",
    "#random forest model\n",
    "model=RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#predictions\n",
    "y_pred=model.predict(x_test)\n",
    "\n",
    "#model evaluation\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "report=classification_report(y_test,y_pred)\n",
    "\n",
    "print(f\"RandomForestClassifier:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB:0.4972568578553616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model=GaussianNB()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "ypred=model.predict(x_test)\n",
    "\n",
    "accuracy=accuracy_score(y_test,ypred)\n",
    "report=classification_report(y_test,ypred)\n",
    "\n",
    "print(f\"Gaussian NB:{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Mean Squared Error:0.250101534996589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model=LinearRegression()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "ypred=model.predict(x_test)\n",
    "\n",
    "mse=mean_squared_error(y_test,ypred)\n",
    "\n",
    "\n",
    "print(f\"Linear Regression Mean Squared Error:{mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:0.49755610972568576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model=LogisticRegression(random_state=46,max_iter=600)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "ypred=model.predict(x_test)\n",
    "\n",
    "accuracy=accuracy_score(y_test,ypred)\n",
    "report=classification_report(y_test,ypred)\n",
    "\n",
    "print(f\"Logistic Regression:{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select K best using Chi square Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:['verified', 'friends_count', 'followers_count', 'gender:confidence']\n",
      "0.5047381546134664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "selector=SelectKBest(score_func=chi2, k='all')\n",
    "x_selected=selector.fit_transform(x,y)\n",
    "\n",
    "selected_features=[features[i] for i in range(len(features)) if selector.get_support()[i]]\n",
    "print(f\"Selected Features:{selected_features}\")\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_selected,y,test_size=0.3,random_state=42,stratify=y)\n",
    "\n",
    "model=LogisticRegression(random_state=42)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "ypred=model.predict(x_test)\n",
    "\n",
    "accuracy=accuracy_score(y_test,ypred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5006 - loss: 0.6947\n",
      "Test Loss: 0.6933 | Test Accuracy: 0.5095\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step\n",
      "Features Selected: ['id', 'verified', 'followers_count', 'friends_count', 'gender:confidence', 'profile_yn:confidence']\n",
      "[[[ 0.93851264 -1.00189706 -0.66193429  0.47916771  1.01862732\n",
      "    0.14372361]]]\n",
      "Predicted Status: Fake\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example Data Loading (Replace 'path_to_your_data.csv' with your actual data path)\n",
    "\n",
    "\n",
    "# Features and target column\n",
    "features = ['id','verified', 'followers_count', 'friends_count', 'gender:confidence', 'profile_yn:confidence']\n",
    "X = data[features].values\n",
    "y = data['status'].values  # Assuming status is the target column\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split the data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "md = models.Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "md.add(layers.LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Add output layer for binary classification\n",
    "md.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "md.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = md.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test),verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = md.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}')\n",
    "tf.get_logger().setLevel('ERROR')  # Suppress warning logs\n",
    "\n",
    "# Example prediction (Predict class: 0 or 1 for fake/real account)\n",
    "sample_input = X_test[0].reshape(1, 1, len(features))  # Reshape for a single sample\n",
    "prediction = md.predict(sample_input,callbacks=None)\n",
    "print(f'Features Selected: {features}')\n",
    "print(f'{X_test[0].reshape(1, 1, len(features))}')\n",
    "print(f'Predicted Status: {\"Real\" if prediction[0][0] > 0.5 else \"Fake\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection using specific ID using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4973 - loss: 0.6939\n",
      "Test Loss: 0.6932 | Test Accuracy: 0.5012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example Data Loading (Replace 'path_to_your_data.csv' with your actual data path)\n",
    "tf.get_logger().setLevel('ERROR')  # Suppress warning logs\n",
    "# Features and target column\n",
    "features = ['verified', 'followers_count', 'friends_count', 'gender:confidence', 'profile_yn:confidence']\n",
    "X = data[features].values\n",
    "y = data['status'].values  # Assuming status is the target column\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split the data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "md = models.Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "md.add(layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Add output layer for binary classification\n",
    "md.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "md.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model without displaying epoch details\n",
    "history = md.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = md.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# --- Prediction for a Specific ID ---\n",
    "# Define the ID you want to query\n",
    "specific_id = 78936667  # Replace with the specific ID number\n",
    "\n",
    "# Find the row for the specific ID\n",
    "if 'id' in data.columns:  # Ensure an 'ID' column exists in the dataset\n",
    "    specific_row = data[data['id'] == specific_id]\n",
    "    if specific_row.empty:\n",
    "        pass\n",
    "    else:\n",
    "        specific_features = specific_row[features].values\n",
    "\n",
    "        # Scale and reshape the data for prediction\n",
    "        specific_features_scaled = scaler.transform(specific_features)\n",
    "        specific_features_reshaped = specific_features_scaled.reshape((specific_features_scaled.shape[0], 1, specific_features_scaled.shape[1]))\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = md.predict(specific_features_reshaped)\n",
    "        print(f\"Predicted Status for ID {specific_id}: {'Real' if prediction[0][0] > 0.5 else 'Fake'}\")\n",
    "else:\n",
    "    print(\"The dataset does not contain an 'ID' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEtection with custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def lstm_with_feature_input():\n",
    "    \n",
    "    # Load the dataset\n",
    "    data=pd.read_csv(\"twitter_dataset.csv\") \n",
    "    try:\n",
    "        print(f\"Dataset loaded successfully! Columns: {data.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Prompt the user for feature columns only\n",
    "    print(\"Target column will be auto-selected as the last column by default.\")\n",
    "    features_input = input(\"Enter the feature columns separated by commas (e.g., 'verified,followers_count,friends_count'): \")\n",
    "    \n",
    "    # Parse the feature column inputs\n",
    "    features = [feature.strip() for feature in features_input.split(',')]\n",
    "    \n",
    "    # Validate feature columns\n",
    "    for col in features:\n",
    "        if col not in data.columns:\n",
    "            print(f\"Error: Column '{col}' is not present in the dataset.\")\n",
    "            return\n",
    "\n",
    "    # Define features and target\n",
    "    X = data[features].values\n",
    "    target_column = data.columns[-1]  # Assuming the target column is the last column\n",
    "    y = data[target_column].values\n",
    "\n",
    "    print(f\"Using features: {features}\")\n",
    "    print(f\"Detected target column: {target_column}\")\n",
    "\n",
    "    # Scaling the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Reshape for LSTM (samples, timesteps, features)\n",
    "    X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "    # Split the data into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    md = models.Sequential()\n",
    "\n",
    "    # Add LSTM layer\n",
    "    md.add(layers.LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "    # Add output layer for binary classification\n",
    "    md.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    md.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = md.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = md.evaluate(X_test, y_test)\n",
    "    print(f'Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}')\n",
    "    tf.get_logger().setLevel('ERROR')  # Suppress warning logs\n",
    "\n",
    "    # Example prediction (Prompt user for custom input)\n",
    "    sample_values = input(f\"Enter values for features {features} separated by commas (e.g., 0,300,150,0.8): \")\n",
    "    try:\n",
    "        sample_input = np.array([float(value.strip()) for value in sample_values.split(',')]).reshape(1, 1, len(features))\n",
    "        prediction = md.predict(sample_input, callbacks=None)\n",
    "        print(f'Predicted Status: {\"Real\" if prediction[0][0] > 0.5 else \"Fake\"}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error with input values: {e}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    lstm_with_feature_input()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
